# Extras

## CUDA Compiler
![](assets/nvcc.png)

## How does CUDA handle conditional if/else logic?
- CUDA does not handle conditional if/else logic well. If you have a conditional statement in your kernel, the compiler will generate code for both branches and then use a predicated instruction to select the correct result. This can lead to a lot of wasted computation if the branches are long or if the condition is rarely met. It is generally a good idea to try to avoid conditional logic in your kernels if possible.
- If it is unavoidable, you can dig down to the PTX assembly code (`nvcc -ptx kernel.cu -o kernel`) and see how the compiler is handling it. Then you can look into the compute metrics of the instructions used and try to optimize it from there.
- Single thread going down a long nested if else statement will look more serialized and leave the other threads waiting for the next instruction while the single threads finishes. this is called **warp divergence** and is a common issue in CUDA programming when dealing with threads specifically within a warp.
- vector addition is fast because divergence isnâ€™t possible, not a different possible way for instructions to carry out.

## Pros and Cons of Unified Memory
- Unified Memory is a feature in CUDA that allows you to allocate memory that is accessible from both the CPU (system DRAM) and the GPU. This can simplify memory management in your code, as you don't have to worry about copying data back and forth between the the RAM sticks and the GPU's memory.
- [Unified vs Explicit Memory in CUDA](https://github.com/lintenn/cudaAddVectors-explicit-vs-unified-memory)
- [Maximizing Unified Memory Performance](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)
- Prefetching is automatically taken care of by unified memory via **streams** (this is what is has lower latency in the github link above)
    - [CUDA streams - Lei Mao](https://leimao.github.io/blog/CUDA-Stream/)
    - [NVIDIA Docs](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution)
    - Streams allow for overlapping data transfer (prefetching) with computation.
    - While one stream is executing a kernel, another stream can be transferring data for the next computation.
    - This technique is often called "double buffering" or "multi-buffering" when extended to more buffers.

![](assets/async.png)

## Memory Architectures
- DRAM/VRAM cells are the smallest unit of memory in a computer. They are made up of capacitors and transistors that store bits of data. The capacitors store the bits as electrical charges, and the transistors control the flow of electricity to read and write the data.
- ![](assets/dram-cell.png)
- SRAM (shared memory) is a type of memory that is faster and more expensive than DRAM. It is used for cache memory in CPUs and GPUs because it can be accessed more quickly than DRAM. 
- Modern NVIDIA GPUs likely use 6T (six-transistor) or 8T SRAM cells for most on-chip memory.
6T cells are compact and offer good performance, while 8T cells can provide better stability and lower power consumption at the cost of larger area.
- 6T vs 8T SRAM cells in NVIDIA GPUs across different architectures and compute capabilities isn't publicly disclosed in detail. NVIDIA, like most semiconductor companies, keeps many of these low-level design choices proprietary.
- ![](assets/sram-cell.png)
- ![](assets/8t-sram-cell.png)


## Dive deeper
- quantization -> fp32 -> fp16 -> int8
- tensor cores (wmma)
- sparsity -> [0, 0, 0, 0, -7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6]
- [CUDA by Example](https://edoras.sdsu.edu/~mthomas/docs/cuda/cuda_by_example.book.pdf)
- [Data-Parallel Distributed Training of Deep Learning Models](https://siboehm.com/articles/22/data-parallel-training)
- [mnist-cudnn](https://github.com/haanjack/mnist-cudnn)
- [CUDA MODE](https://github.com/cuda-mode/lectures)
- [micrograd-cuda](https://github.com/mlecauchois/micrograd-cuda)
- [micrograd](https://github.com/karpathy/micrograd)
- [GPU puzzles](https://github.com/srush/GPU-Puzzles)


ä»¥ä¸‹æ˜¯ä½ æä¾›å†…å®¹çš„**å®Œæ•´ä¸­æ–‡ç¿»è¯‘ä¸æ•´ç†æ³¨è§£**ï¼š

---

# ğŸŒŸ é™„åŠ è¯´æ˜ï¼ˆExtrasï¼‰

---

## âœ… CUDA ç¼–è¯‘å™¨

* `nvcc` æ˜¯ NVIDIA æä¾›çš„ CUDA ç¼–è¯‘å™¨ï¼Œç”¨äºå°† `.cu` æ–‡ä»¶ç¼–è¯‘ä¸ºå¯ä¾› GPU æ‰§è¡Œçš„ PTX æˆ–äºŒè¿›åˆ¶ä»£ç ã€‚

---

## â“CUDA å¦‚ä½•å¤„ç† if/else æ¡ä»¶é€»è¾‘ï¼Ÿ

* CUDA **å¹¶ä¸æ“…é•¿å¤„ç†æ¡ä»¶åˆ†æ”¯é€»è¾‘**ã€‚
  å¦‚æœ kernel ä¸­åŒ…å« `if/else` è¯­å¥ï¼Œç¼–è¯‘å™¨é€šå¸¸ä¼šï¼š

  * ä¸º **æ¯ä¸ªåˆ†æ”¯éƒ½ç”Ÿæˆä»£ç **
  * å¹¶é€šè¿‡ **è°“è¯åŒ–æŒ‡ä»¤**ï¼ˆpredicated instructionï¼‰æ¥é€‰æ‹©å®é™…æ‰§è¡Œçš„è·¯å¾„

### âš ï¸ è¿™æ ·åšå¯èƒ½å¯¼è‡´çš„é—®é¢˜ï¼š

* **åˆ†æ”¯é€»è¾‘è¶Šå¤æ‚æˆ–æ»¡è¶³æ¡ä»¶çš„çº¿ç¨‹è¶Šå°‘æ—¶ï¼Œæ€§èƒ½æŸè€—è¶Šå¤§**
* **æµªè´¹äº†å¤§é‡æœªä½¿ç”¨åˆ†æ”¯çš„è®¡ç®—èµ„æº**

### ğŸ” è°ƒè¯•å»ºè®®ï¼š

* ä½¿ç”¨ `nvcc -ptx kernel.cu -o kernel.ptx` ç”Ÿæˆ PTX æ±‡ç¼–ï¼Œæ£€æŸ¥å®é™…ç¼–è¯‘ç»“æœ
* æŸ¥çœ‹ç¼–è¯‘åæ˜¯å¦å‡ºç°ä¸å¿…è¦çš„å†—ä½™åˆ†æ”¯è·¯å¾„

### ğŸš§ Warp Divergenceï¼ˆçº¿ç¨‹æŸåˆ†æ­§ï¼‰

* å½“ä¸€ä¸ª warpï¼ˆ32 ä¸ªçº¿ç¨‹ï¼‰ä¸­æœ‰éƒ¨åˆ†çº¿ç¨‹æ‰§è¡Œäº†ä¸åŒçš„åˆ†æ”¯è·¯å¾„ï¼Œ**å…¶ä½™çº¿ç¨‹å¿…é¡»ç­‰å¾…**
  â†’ **è¿™ä¼šå¯¼è‡´çº¿ç¨‹æŸä¸­çº¿ç¨‹ä¸²è¡Œæ‰§è¡Œï¼Œå½±å“å¹¶è¡Œæ€§**
* æ¯”å¦‚ä¸€ä¸ªçº¿ç¨‹æ·±åº¦åµŒå¥—çš„ if-elseï¼Œå…¶å®ƒçº¿ç¨‹å°±å¾—ç­‰å¾…å®ƒæ‰§è¡Œå®Œ

âœ… ä¾‹å¦‚å‘é‡åŠ æ³•ï¼ˆvector additionï¼‰æ²¡æœ‰æ¡ä»¶åˆ†æ”¯ï¼Œå› æ­¤è¿è¡Œæ•ˆç‡éå¸¸é«˜ã€‚

---

## âœ… Unified Memoryï¼ˆç»Ÿä¸€å†…å­˜ï¼‰çš„ä¼˜åŠ£åŠ¿

### ğŸ“Œ å®šä¹‰

* ç»Ÿä¸€å†…å­˜æ˜¯ CUDA æä¾›çš„ä¸€ç§æœºåˆ¶ï¼Œå…è®¸åˆ†é…çš„å†…å­˜åœ¨ CPU å’Œ GPU é—´å…±äº«è®¿é—®ã€‚
* ç”¨æˆ·ä¸å†éœ€è¦æ˜¾å¼åœ°åœ¨ host/device ä¹‹é—´å¤åˆ¶æ•°æ®ã€‚

### âœ… ä¼˜ç‚¹

* **ç®€åŒ–å†…å­˜ç®¡ç†**ï¼šæ— éœ€è°ƒç”¨ `cudaMemcpy`
* **è‡ªåŠ¨é¢„å–**ï¼ˆprefetchï¼‰ï¼šCUDA ä¼šåœ¨åˆé€‚æ—¶æœºä½¿ç”¨ stream å°†æ•°æ®è‡ªåŠ¨è¿ç§»

### âš ï¸ ç¼ºç‚¹

* æ€§èƒ½å¯é¢„æµ‹æ€§å·®ï¼Œ**ä¸å½“ä½¿ç”¨ä¼šå¯¼è‡´å»¶è¿Ÿ**æˆ– page fault
* åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œæ‰‹åŠ¨ç®¡ç†æ˜¾å­˜å¯èƒ½æ›´é«˜æ•ˆ

### ğŸ“š æ¨èèµ„æ–™

* [Unified vs Explicit Memory in CUDA (GitHub ç¤ºä¾‹)](https://github.com/lintenn/cudaAddVectors-explicit-vs-unified-memory)
* [å®˜æ–¹ï¼šæœ€å¤§åŒ–ç»Ÿä¸€å†…å­˜æ€§èƒ½](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/)

---

## ğŸš€ CUDA Streams ä¸å¼‚æ­¥æ•°æ®é¢„å–

### æ¦‚å¿µ

* CUDA stream å…è®¸å¤šä¸ª kernel æˆ–æ•°æ®ä¼ è¾“ **å¹¶å‘æ‰§è¡Œ**ã€‚
* ä¸€èˆ¬å¯ä½¿ç”¨ã€ŒåŒç¼“å†²ã€ã€Œå¤šç¼“å†²ã€ç­‰æ–¹å¼å®ç°è®¡ç®—ä¸æ•°æ®ä¼ è¾“é‡å ã€‚

### æ¨èé˜…è¯»

* [Lei Mao çš„ stream æ•™ç¨‹](https://leimao.github.io/blog/CUDA-Stream/)
* [NVIDIA å®˜æ–¹æ–‡æ¡£ï¼šå¼‚æ­¥æ‰§è¡Œ](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution)

---

## ğŸ§  å†…å­˜æ¶æ„æ¦‚è¿°ï¼ˆMemory Architecturesï¼‰

### 1. DRAM / VRAM

* ç”±ç”µå®¹ä¸æ™¶ä½“ç®¡ç»„æˆï¼Œæ¯ä¸ª cell å­˜å‚¨ 1 bit
* æˆæœ¬ä½ä½†è®¿é—®é€Ÿåº¦æ…¢
* GPU å…¨å±€å†…å­˜ä½¿ç”¨ DRAM æŠ€æœ¯

å›¾ç¤ºï¼š

* ![DRAM cell](assets/dram-cell.png)

---

### 2. SRAMï¼ˆå…±äº«å†…å­˜ / å¯„å­˜å™¨ç¼“å­˜ï¼‰

* ç”¨äº CPU/GPU ç¼“å­˜ï¼ˆL1/L2 cacheã€shared memoryï¼‰

* æ¯” DRAM å¿«å¾ˆå¤šï¼Œä½†æˆæœ¬é«˜

* **6T SRAMï¼ˆ6 ä¸ªæ™¶ä½“ç®¡ï¼‰**ï¼š

  * æ›´ç´§å‡‘ï¼Œå ç”¨èŠ¯ç‰‡é¢ç§¯å°ï¼Œé€Ÿåº¦å¿«

* **8T SRAM**ï¼š

  * æ›´ç¨³å®šã€åŠŸè€—ä½ï¼Œä½†å ç”¨é¢ç§¯æ›´å¤§

å›¾ç¤ºï¼š

* ![SRAM cell](assets/sram-cell.png)
* ![8T SRAM cell](assets/8t-sram-cell.png)

> ğŸ’¡ NVIDIA å®é™…ä½¿ç”¨å“ªç§ SRAM è®¾è®¡æœªå®Œå…¨å…¬å¼€ï¼Œä½†ä¸»è¦ä¸ºç‰‡ä¸Šé«˜é€Ÿç¼“å­˜ä½¿ç”¨ 6T/8T ç»“æ„ã€‚

---

## ğŸ“š æ·±å…¥æ¢ç´¢æ–¹å‘ï¼ˆDive deeperï¼‰

| ä¸»é¢˜                | è¯´æ˜                                                                 |
| ----------------- | ------------------------------------------------------------------ |
| é‡åŒ– (Quantization) | ä» float32 â†’ float16 â†’ int8ï¼Œç‰ºç‰²ç²¾åº¦ä»¥åŠ é€Ÿæ¨ç†                               |
| Tensor Core       | GPU çŸ©é˜µä¹˜ä¸“ç”¨ç¡¬ä»¶ï¼Œæ”¯æŒåŠç²¾åº¦ä¸ç¨€ç–è®¡ç®—                                             |
| ç¨€ç–æ€§ (Sparsity)    | åœ¨æ¨¡å‹ä¸­å¤§é‡ä¸º 0 çš„ä½ç½®å¯è¢«ä¼˜åŒ–è·³è¿‡ï¼Œå¦‚ `[0, 0, ..., 6]`                             |
| åˆ†å¸ƒå¼è®­ç»ƒ             | [æ•°æ®å¹¶è¡Œè®­ç»ƒåˆ†æ](https://siboehm.com/articles/22/data-parallel-training) |

---

## ğŸ”— æ¨èé¡¹ç›®ä¸èµ„æ–™

* ğŸ“˜ [CUDA by Example](https://edoras.sdsu.edu/~mthomas/docs/cuda/cuda_by_example.book.pdf)
* ğŸ” [Explicit vs Unified Memory ç¤ºä¾‹](https://github.com/lintenn/cudaAddVectors-explicit-vs-unified-memory)
* ğŸ”¬ [mnist-cudnnï¼ˆå·ç§¯åŠ é€Ÿç¤ºä¾‹ï¼‰](https://github.com/haanjack/mnist-cudnn)
* ğŸ“ [CUDA MODE æ•™ç¨‹é›†åˆ](https://github.com/cuda-mode/lectures)
* ğŸ§  [micrograd-cudaï¼ˆKarpathy å¾®å‹æ¡†æ¶çš„ CUDA å®ç°ï¼‰](https://github.com/mlecauchois/micrograd-cuda)
* ğŸ§  [microgradï¼ˆåŸç‰ˆ Py å®ç°ï¼‰](https://github.com/karpathy/micrograd)
* ğŸ® [GPU ç›Šæ™ºé¢˜](https://github.com/srush/GPU-Puzzles) - ç”¨äºç»ƒä¹  CUDA æ€ç»´æ¨¡å‹çš„æŒ‘æˆ˜é¢˜

---

å¦‚éœ€æˆ‘ä¸ºä½ æŠŠè¿™äº›å†…å®¹è½¬ä¸º Markdown å­¦ä¹ ç¬”è®°ã€å¹»ç¯ç‰‡æˆ–æ•´ç†æˆå¯æ‰“å° PDFï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ã€‚
